{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# kaggle standard imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n\n# extra imports\nnp.random.seed(235)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nimport gc\nimport re\nfrom sklearn.metrics import f1_score\n\n# XGboost related\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  #tfidf\nfrom scipy.sparse import csr_matrix, hstack #Compressed Sparse Row format\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ae54c723c6ec2e10f6de63d50827d8c4ea6d894"},"cell_type":"code","source":"print('load data') \n# load training and testing data\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\n\n# split training data to validation\ntrain_df, val_df = train_test_split(train_df, train_size=0.9, random_state=235)\nprint(\"train_df shape= \", train_df.shape)\nprint(\"val_df shape=\",val_df.shape)\nprint(\"load data success !\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2d6bb2f0ec0dfe1b4a2b44b4c03e2ff473ab4ce"},"cell_type":"code","source":"print('fill missing and get the values')\n# fill missing and get the values\nX_train = train_df[\"question_text\"].fillna(\"na_\").values\nX_val = val_df[\"question_text\"].fillna(\"na_\").values\nX_test = test_df[\"question_text\"].fillna(\"na_\").values\n\ny_train = train_df['target'].values\ny_val = val_df['target'].values\n#print(X_train[:12])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"print('size of training data: ', X_train.shape)  # size of training data:  (1175509,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d0873c3194016b7a6de3ab771673da9345dbfc2"},"cell_type":"code","source":"char_vector = TfidfVectorizer(\n    ngram_range=(2,4),     # 对ngram进行TFIDF\n    max_features=20000,\n    stop_words='english',   # list类型\n    analyzer='char_wb',\n    token_pattern=r'\\w{1,}',\n    strip_accents='unicode',\n    sublinear_tf=True, \n    max_df=0.98,\n    min_df=2  # 上下词频阈值之外的单词不计入\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cc90859382b2f903aedf66c3310e6b9a9c416eb"},"cell_type":"code","source":"print('fit char vector')\nchar_vector.fit(X_train[:85000])  # ?为什么只取训练集中的一部分还不清楚\nprint(\"fit success !\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e77f4223e68a3fa0a1734171bb3c2a53837ba58"},"cell_type":"code","source":"print('transfer data based on char vector')\nprint('transfer train')\n# 返回TFIDF矩阵。tocsr存储稀疏矩阵\ntrain_char_vector = char_vector.transform(X_train).tocsr() \nprint('transfer validation')\nvalid_char_vector = char_vector.transform(X_val).tocsr()\nprint('transfer test')\ntest_char_vector = char_vector.transform(X_test).tocsr()\n\nprint(\"finished !\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ab99590e76259b74002c7a6fd3e7370bdeb8ee4"},"cell_type":"code","source":"all_text = list(X_train) + list(X_test)  # 训练集+测试集。但是没有包含验证集\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a01571b0cc8ef816b9a063c441b723fdbd928de"},"cell_type":"code","source":"word_vector = TfidfVectorizer(\n    ngram_range=(1,1),  # 对每一个单词进行TFIDF\n    max_features=9000,\n    sublinear_tf=True, \n    strip_accents='unicode', \n    analyzer='word', \n    token_pattern=\"\\w{1,}\", \n    stop_words=\"english\",\n    max_df=0.95,\n    min_df=2\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5690d6ac8fdfdc3df240ecf32b0cd0fce29991db"},"cell_type":"code","source":"print('fit word vector')\nword_vector.fit(all_text)\nprint(\"finished!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52c6bae6cf210dfec8399ba4722f54f599232efd"},"cell_type":"code","source":"print('transfer data based on word vector')\n# transform后得到tfidf矩阵。toser对稀疏矩阵压缩存储\ntrain_word_vector = word_vector.transform(X_train).tocsr()\nvalid_word_vector = word_vector.transform(X_val).tocsr()\ntest_word_vector = word_vector.transform(X_test).tocsr()\nprint(\"finished!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d327cc553e59472be680c4f908761777f4144e1c"},"cell_type":"code","source":"del all_text\ndel X_train\ndel X_val\ndel X_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"906a89d4055f764556f4be6024697682aa1e2456"},"cell_type":"code","source":"data = [train_df, val_df, test_df]\nprint(\"finished!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88bb801549bde853ad1ab18741a23ed88e39e5b6"},"cell_type":"code","source":"# references: https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing\nmistake_list = ['colour', 'centre', 'favourite', 'travelling', 'counselling', 'theatre', 'cancelled', 'labour', 'organisation', 'wwii', 'citicise', 'youtu ', 'youtube ', 'Qoura', 'sallary', 'Whta', 'narcisist', 'howdo', 'whatare', 'howcan', 'howmuch', 'howmany', 'whydo', 'doI', 'theBest', 'howdoes', 'mastrubation', 'mastrubate', \"mastrubating\", 'pennis', 'Etherium', 'narcissit', 'bigdata', '2k17', '2k18', 'qouta', 'exboyfriend', 'airhostess', 'whst', 'watsapp', 'demonitisation', 'demonitization', 'demonetisation']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9d5db59fb4985392f6e72f50ee90c0cc24aff40"},"cell_type":"code","source":"def get_features(data):\n    # data = [train_df, val_df, test_df]  (3,)\n    # 经过本函数的处理后 data的第一维[1175509 rows x 3 columns]变为[1175509 rows x 16 columns]\n    # 第二维由[130613 rows x 3 columns] 变为 [130613 rows x 16 columns]\n    # 第三维由[56370 rows x 2 columns] 变为 [56370 rows x 15 columns]\n    for dataframe in data:\n        # dataFrame 添加列\n        dataframe[\"text_size\"] = dataframe[\"question_text\"].apply(len).astype('uint16')  # 句子长度\n        dataframe[\"capital_size\"] = dataframe[\"question_text\"].apply(lambda x: sum(1 for c in x if c.isupper())).astype('uint16')  # 大写字母的个数\n        dataframe[\"capital_rate\"] = dataframe.apply(lambda x: float(x[\"capital_size\"]) / float(x[\"text_size\"]), axis=1).astype('float16')  # 大写字母率\n        dataframe[\"exc_count\"] = dataframe[\"question_text\"].apply(lambda x: x.count(\"!\")).astype('uint16')  # 感叹号数量\n        dataframe[\"quetion_count\"] = dataframe[\"question_text\"].apply(lambda x: x.count(\"?\")).astype('uint16')  # 问号数量\n        dataframe[\"unq_punctuation_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(p) for p in '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²')).astype('uint16') # 不同标点符号数量\n        dataframe[\"punctuation_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(p) for p in '.,;:^_`')).astype('uint16')  # 标点符号数量\n        dataframe[\"symbol_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(p) for p in '*&$%')).astype('uint16')  # ？？\n        dataframe[\"words_count\"] = dataframe[\"question_text\"].apply(lambda x: len(x.split())).astype('uint16')  # 单词数量\n        dataframe[\"unique_words\"] = dataframe[\"question_text\"].apply(lambda x: (len(set(1 for w in x.split())))).astype('uint16')  # 不同单词的数量\n        dataframe[\"unique_rate\"] = dataframe[\"unique_words\"] / dataframe[\"words_count\"]  \n        dataframe[\"word_max_length\"] = dataframe[\"question_text\"].apply(lambda x: max([len(word) for word in x.split()]) ).astype('uint16')  # 最大单词长度\n        dataframe[\"mistake_count\"] = dataframe[\"question_text\"].apply(lambda x: sum(x.count(w) for w in mistake_list)).astype('uint16')  # 错误拼写数量\n    print(\"data shape = \", np.array(data).shape)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8996d32543ef1ede4411c63cb295fcc0bfe1b707"},"cell_type":"code","source":"print('generate the features')\n# data = [train_df, val_df, test_df]\ndata = get_features(data)\n# print(\"data shape = \", np.array(data).shape)\n# print(data)\nprint(\"finished!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f093cb6970242fffa80d2da09f694414edadfe2e"},"cell_type":"code","source":"feature_cols = [\"text_size\", \"capital_size\", \"capital_rate\", \"exc_count\", \"quetion_count\", \"unq_punctuation_count\", \"punctuation_count\", \"symbol_count\", \"words_count\", \"unique_words\", \"unique_rate\", \"word_max_length\", \"mistake_count\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7c8af39e09872713cbaa943a1580098945bfc0d"},"cell_type":"code","source":"print('final preparation for input')\n# 不取qid，question text，target列的数据，只取feature_cols列的数据。\nX_train = csr_matrix(train_df[feature_cols].values)\nX_val = csr_matrix(val_df[feature_cols].values)\nX_test = csr_matrix(test_df[feature_cols].values)\n\ndel val_df\ndel train_df\ndel test_df\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"590bec3cf1462646b074ec90793e81f949db933f"},"cell_type":"code","source":"'''\ninput_train = hstack([X_train, train_char_vector,train_word_vector])\ninput_valid = hstack([X_val, valid_char_vector, valid_word_vector])\ninput_test = hstack([X_test, test_char_vector, test_word_vector])\n'''\n# 按列将数组堆叠\ninput_train = hstack([X_train, train_word_vector, train_char_vector])\ninput_valid = hstack([X_val, valid_word_vector, valid_char_vector])\ninput_test = hstack([X_test, test_word_vector, test_char_vector])\n\n#print('input_train: ', input_train)\ntrain_word_vector = None\ntrain_char_vector = None\nvalid_word_vector = None\nvalid_char_vector = None\ntest_word_vector = None\ntest_char_vector = None\n#print('input_train: ', input_train)\nprint(\"finished!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d74cdc98ac31abd869418ed01ac762eefcf5d9b"},"cell_type":"code","source":"import json\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import  make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport pickle  \nprint('Load data...')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1376977281d1824de17a3b33a76a2116034edfc"},"cell_type":"code","source":"# create dataset for lightgbm  \nlgb_train = lgb.Dataset(input_train, y_train )  \nlgb_eval = lgb.Dataset(input_valid, y_val, reference=lgb_train)  \n# specify your configurations as a dict  \nparams = {  \n    'boosting_type': 'gbdt',  \n    'objective': 'binary',  \n    'metric': {'binary_logloss', 'auc'},  #二进制对数损失\n    'num_leaves': 5,  \n    'max_depth': 6,  \n    'min_data_in_leaf': 450,  \n    'learning_rate': 0.1,  \n    'feature_fraction': 0.9,  \n    'bagging_fraction': 0.95,  \n    'bagging_freq': 5,  \n    'lambda_l1': 1,    \n    'lambda_l2': 0.001,  # 越小l2正则程度越高  \n    'min_gain_to_split': 0.2,  \n    'verbose': 5,  \n    'is_unbalance': True  \n}  \n\n# train  \nprint('Start training...')  \ngbm = lgb.train(params,  \n                lgb_train,  \n                num_boost_round=10000,  \n                valid_sets=lgb_eval,  \n                early_stopping_rounds=500)  \n\nprint('Start predicting...')  \n\npreds = gbm.predict(input_valid, num_iteration=gbm.best_iteration)  # 输出的是概率结果  \n\n# 导出结果  \nscores_list = []\nfor threshold in [0.2, 0.3, 0.31, 0.33, 0.4, 0.45, 0.5]:\n    score = f1_score(y_val, (preds > threshold).astype(int))\n    scores_list.append([threshold, score])\n    print('F1 score: {} for threshold: {}'.format(score, threshold))\n        \n    scores_list.sort(key=lambda x:x[1] , reverse=True)\n    best_threshold = scores_list[0][0]\n    print('best threshold to generate predictions: ', best_threshold)\n    print('best score: ', scores_list[0][1])\n# 导出特征重要性  \nimportance = gbm.feature_importance()  \nnames = gbm.feature_name()  \nwith open('./feature_importance.txt', 'w+') as file:  \n    for index, im in enumerate(importance):  \n        string = names[index] + ', ' + str(im) + '\\n'  \n        file.write(string)  \n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}